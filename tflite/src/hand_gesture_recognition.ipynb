{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "filename = \"punch_1.csv\"\n",
    "\n",
    "df = pd.read_csv(\"../include/\" + filename)\n",
    "# print(df.dtypes)\n",
    "\n",
    "index = range(1, len(df[\"aX\"]) + 1)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 10)\n",
    "\n",
    "plt.plot(index, df[\"aX\"], \"g.\", label=\"x\", linestyle=\"solid\", marker=\",\")\n",
    "plt.plot(index, df[\"aY\"], \"b.\", label=\"y\", linestyle=\"solid\", marker=\",\")\n",
    "plt.plot(index, df[\"aZ\"], \"r.\", label=\"z\", linestyle=\"solid\", marker=\",\")\n",
    "plt.title(\"Acceleration\")\n",
    "plt.xlabel(\"Sample #\")\n",
    "plt.ylabel(\"Acceleration (G)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\quoct\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "TensorFlow version = 2.15.0\n",
      "\n",
      "Processing index 0 for gesture 'punch'.\n",
      "    aX    aY   aZ\n",
      "0 -715 -1381 -788\n",
      "1 -715 -1381 -788\n",
      "2 -715 -1381 -788\n",
      "3 -715 -1381 -788\n",
      "4 -715 -1381 -788\n",
      "(1080, 3)\n",
      "1080  --  119\n",
      "\tThere are 9 batches of the *punch* gesture.\n",
      "\n",
      "Processing index 1 for gesture 'flex'.\n",
      "     aX   aY   aZ\n",
      "0 -1497  138 -408\n",
      "1 -1497  138 -408\n",
      "2 -1497  138 -408\n",
      "3 -1497  138 -408\n",
      "4 -1469  155 -356\n",
      "(1080, 3)\n",
      "1080  --  119\n",
      "\tThere are 9 batches of the *flex* gesture.\n",
      "\n",
      "Data set parsing and preparation complete.\n",
      "18 \n",
      "\n",
      "punch records length per batch = len([ax,ay,az]) * TOTAL_RECORDS_PER_BATCH = 3 * TOTAL_RECORDS_PER_BATCH =  357 \n",
      "\n",
      "punch input:  [0.32543945 0.1628418  0.30761719 0.32543945 0.1628418  0.30761719\n",
      " 0.32543945 0.1628418  0.30761719 0.32543945 0.1628418  0.30761719\n",
      " 0.32543945 0.1628418  0.30761719 0.32543945 0.1628418  0.30761719\n",
      " 0.32543945 0.1628418  0.30761719 0.32543945 0.1628418  0.30761719\n",
      " 0.21899414 0.09106445 0.40698242 0.21899414 0.09106445 0.40698242\n",
      " 0.21899414 0.09106445 0.40698242 0.21899414 0.09106445 0.40698242\n",
      " 0.14111328 0.06079102 0.45336914 0.14111328 0.06079102 0.45336914\n",
      " 0.14111328 0.06079102 0.45336914 0.14111328 0.06079102 0.45336914\n",
      " 0.14111328 0.06079102 0.45336914 0.14111328 0.06079102 0.45336914\n",
      " 0.14111328 0.06079102 0.45336914 0.14111328 0.06079102 0.45336914\n",
      " 0.14111328 0.06079102 0.45336914 0.14111328 0.06079102 0.45336914\n",
      " 0.09716797 0.00927734 0.50585938 0.09716797 0.00927734 0.50585938\n",
      " 0.09716797 0.00927734 0.50585938 0.09716797 0.00927734 0.50585938\n",
      " 0.1315918  0.00610352 0.45019531 0.1315918  0.00610352 0.45019531\n",
      " 0.1315918  0.00610352 0.45019531 0.1315918  0.00610352 0.45019531\n",
      " 0.1315918  0.00610352 0.45019531 0.1315918  0.00610352 0.45019531\n",
      " 0.1315918  0.00610352 0.45019531 0.27929688 0.07617188 0.37963867\n",
      " 0.27929688 0.07617188 0.37963867 0.27929688 0.07617188 0.37963867\n",
      " 0.27929688 0.07617188 0.37963867 0.27929688 0.07617188 0.37963867\n",
      " 0.27929688 0.07617188 0.37963867 0.27929688 0.07617188 0.37963867\n",
      " 0.27929688 0.07617188 0.37963867 0.27929688 0.07617188 0.37963867\n",
      " 0.27929688 0.07617188 0.37963867 0.27929688 0.07617188 0.37963867\n",
      " 0.47583008 0.23706055 0.13623047 0.47583008 0.23706055 0.13623047\n",
      " 0.47583008 0.23706055 0.13623047 0.47583008 0.23706055 0.13623047\n",
      " 0.47583008 0.23706055 0.13623047 0.47583008 0.23706055 0.13623047\n",
      " 0.47583008 0.23706055 0.13623047 0.47583008 0.23706055 0.13623047\n",
      " 0.47583008 0.23706055 0.13623047 0.47583008 0.23706055 0.13623047\n",
      " 0.47583008 0.23706055 0.13623047 0.46435547 0.40185547 0.08422852\n",
      " 0.46435547 0.40185547 0.08422852 0.46435547 0.40185547 0.08422852\n",
      " 0.46435547 0.40185547 0.08422852 0.46435547 0.40185547 0.08422852\n",
      " 0.46435547 0.40185547 0.08422852 0.48583984 0.64892578 0.17236328\n",
      " 0.48583984 0.64892578 0.17236328 0.48583984 0.64892578 0.17236328\n",
      " 0.48583984 0.64892578 0.17236328 0.48583984 0.64892578 0.17236328\n",
      " 0.45483398 0.88989258 0.30908203 0.45483398 0.88989258 0.30908203\n",
      " 0.45483398 0.88989258 0.30908203 0.45483398 0.88989258 0.30908203\n",
      " 0.45483398 0.88989258 0.30908203 0.45483398 0.88989258 0.30908203\n",
      " 0.45483398 0.88989258 0.30908203 0.45483398 0.88989258 0.30908203\n",
      " 0.45483398 0.88989258 0.30908203 0.45483398 0.88989258 0.30908203\n",
      " 0.45483398 0.88989258 0.30908203 0.45483398 0.88989258 0.30908203\n",
      " 0.43261719 0.99975586 0.33178711 0.52587891 0.99975586 0.21850586\n",
      " 0.52587891 0.99975586 0.21850586 0.52587891 0.99975586 0.21850586\n",
      " 0.52587891 0.99975586 0.21850586 0.52587891 0.99975586 0.21850586\n",
      " 0.52587891 0.99975586 0.21850586 0.52587891 0.99975586 0.21850586\n",
      " 0.52587891 0.99975586 0.21850586 0.52587891 0.99975586 0.21850586\n",
      " 0.52587891 0.99975586 0.21850586 0.52587891 0.99975586 0.21850586\n",
      " 0.6706543  0.99731445 0.08105469 0.6706543  0.99731445 0.08105469\n",
      " 0.6706543  0.99731445 0.08105469 0.6706543  0.99731445 0.08105469\n",
      " 0.6706543  0.99731445 0.08105469 0.6706543  0.99731445 0.08105469\n",
      " 0.6706543  0.99731445 0.08105469 0.6706543  0.99731445 0.08105469\n",
      " 0.89501953 0.99975586 0.00292969 0.89501953 0.99975586 0.00292969\n",
      " 0.89501953 0.99975586 0.00292969 0.89501953 0.99975586 0.00292969\n",
      " 0.99975586 0.87915039 0.09912109 0.99975586 0.87915039 0.09912109\n",
      " 0.99975586 0.87915039 0.09912109 0.99975586 0.87915039 0.09912109\n",
      " 0.99975586 0.87915039 0.09912109 0.99975586 0.87915039 0.09912109\n",
      " 0.99975586 0.87915039 0.09912109 0.99975586 0.87915039 0.09912109\n",
      " 0.99975586 0.87915039 0.09912109 0.99975586 0.87915039 0.09912109\n",
      " 0.93994141 0.25708008 0.52026367 0.93994141 0.25708008 0.52026367\n",
      " 0.93994141 0.25708008 0.52026367 0.93994141 0.25708008 0.52026367\n",
      " 0.93994141 0.25708008 0.52026367 0.50708008 0.0546875  0.48413086\n",
      " 0.50708008 0.0546875  0.48413086]  output:  [1. 0.]\n",
      "flex input:  [0.50708008 0.0546875  0.48413086 0.39331055 0.234375   0.26464844\n",
      " 0.39331055 0.234375   0.26464844 0.39331055 0.234375   0.26464844\n",
      " 0.40087891 0.20703125 0.25634766 0.40087891 0.20703125 0.25634766\n",
      " 0.40087891 0.20703125 0.25634766 0.40087891 0.20703125 0.25634766\n",
      " 0.40087891 0.20703125 0.25634766 0.40087891 0.20703125 0.25634766\n",
      " 0.40087891 0.20703125 0.25634766 0.40087891 0.20703125 0.25634766\n",
      " 0.40087891 0.20703125 0.25634766 0.40722656 0.14379883 0.27075195\n",
      " 0.40722656 0.14379883 0.27075195 0.40722656 0.14379883 0.27075195\n",
      " 0.40722656 0.14379883 0.27075195 0.40722656 0.14379883 0.27075195\n",
      " 0.40722656 0.14379883 0.27075195 0.40722656 0.14379883 0.27075195\n",
      " 0.40722656 0.14379883 0.27075195 0.40722656 0.14379883 0.27075195\n",
      " 0.39746094 0.07470703 0.29296875 0.39746094 0.07470703 0.29296875\n",
      " 0.41357422 0.06640625 0.3449707  0.41357422 0.06640625 0.3449707\n",
      " 0.41357422 0.06640625 0.3449707  0.41357422 0.06640625 0.3449707\n",
      " 0.41357422 0.06640625 0.3449707  0.41357422 0.06640625 0.3449707\n",
      " 0.41357422 0.06640625 0.3449707  0.41357422 0.06640625 0.3449707\n",
      " 0.41357422 0.06640625 0.3449707  0.41357422 0.06640625 0.3449707\n",
      " 0.41357422 0.06640625 0.3449707  0.38818359 0.08032227 0.44165039\n",
      " 0.38818359 0.08032227 0.44165039 0.38818359 0.08032227 0.44165039\n",
      " 0.38818359 0.08032227 0.44165039 0.26660156 0.07836914 0.58642578\n",
      " 0.26660156 0.07836914 0.58642578 0.26660156 0.07836914 0.58642578\n",
      " 0.26660156 0.07836914 0.58642578 0.26660156 0.07836914 0.58642578\n",
      " 0.26660156 0.07836914 0.58642578 0.26660156 0.07836914 0.58642578\n",
      " 0.26660156 0.07836914 0.58642578 0.13012695 0.08251953 0.66796875\n",
      " 0.13012695 0.08251953 0.66796875 0.13012695 0.08251953 0.66796875\n",
      " 0.13012695 0.08251953 0.66796875 0.13012695 0.08251953 0.66796875\n",
      " 0.13012695 0.08251953 0.66796875 0.13012695 0.08251953 0.66796875\n",
      " 0.13012695 0.08251953 0.66796875 0.13012695 0.08251953 0.66796875\n",
      " 0.13012695 0.08251953 0.66796875 0.13012695 0.08251953 0.66796875\n",
      " 0.26049805 0.45947266 0.44775391 0.26049805 0.45947266 0.44775391\n",
      " 0.26049805 0.45947266 0.44775391 0.26049805 0.45947266 0.44775391\n",
      " 0.26049805 0.45947266 0.44775391 0.26049805 0.45947266 0.44775391\n",
      " 0.26049805 0.45947266 0.44775391 0.26049805 0.45947266 0.44775391\n",
      " 0.26049805 0.45947266 0.44775391 0.26049805 0.45947266 0.44775391\n",
      " 0.26049805 0.45947266 0.44775391 0.26049805 0.45947266 0.44775391\n",
      " 0.46728516 0.72314453 0.21337891 0.46728516 0.72314453 0.21337891\n",
      " 0.46728516 0.72314453 0.21337891 0.46728516 0.72314453 0.21337891\n",
      " 0.46728516 0.72314453 0.21337891 0.6171875  0.95678711 0.01928711\n",
      " 0.6171875  0.95678711 0.01928711 0.6171875  0.95678711 0.01928711\n",
      " 0.6171875  0.95678711 0.01928711 0.6171875  0.95678711 0.01928711\n",
      " 0.6171875  0.95678711 0.01928711 0.6171875  0.95678711 0.01928711\n",
      " 0.62597656 0.99975586 0.         0.62597656 0.99975586 0.\n",
      " 0.62597656 0.99975586 0.         0.62597656 0.99975586 0.\n",
      " 0.62597656 0.99975586 0.         0.62597656 0.99975586 0.\n",
      " 0.62597656 0.99975586 0.         0.62597656 0.99975586 0.\n",
      " 0.62597656 0.99975586 0.         0.62597656 0.99975586 0.\n",
      " 0.62597656 0.99975586 0.         0.57397461 0.99829102 0.078125\n",
      " 0.59985352 0.99853516 0.06591797 0.59985352 0.99853516 0.06591797\n",
      " 0.59985352 0.99853516 0.06591797 0.59985352 0.99853516 0.06591797\n",
      " 0.59985352 0.99853516 0.06591797 0.59985352 0.99853516 0.06591797\n",
      " 0.59985352 0.99853516 0.06591797 0.59985352 0.99853516 0.06591797\n",
      " 0.59985352 0.99853516 0.06591797 0.59985352 0.99853516 0.06591797\n",
      " 0.59985352 0.99853516 0.06591797 0.89404297 0.99975586 0.\n",
      " 0.89404297 0.99975586 0.         0.89404297 0.99975586 0.\n",
      " 0.89404297 0.99975586 0.         0.89404297 0.99975586 0.\n",
      " 0.89404297 0.99975586 0.         0.89404297 0.99975586 0.\n",
      " 0.89404297 0.99975586 0.         0.99975586 0.79467773 0.43823242\n",
      " 0.99975586 0.79467773 0.43823242 0.99975586 0.79467773 0.43823242\n",
      " 0.68017578 0.09155273 0.78857422 0.68017578 0.09155273 0.78857422\n",
      " 0.68017578 0.09155273 0.78857422]  output:  [1. 0.]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "print(f\"TensorFlow version = {tf.__version__}\\n\")\n",
    "\n",
    "# Set a fixed random seed value, for reproducibility, this will allow us to get\n",
    "# the same random numbers each time the notebook is run\n",
    "SEED = 1337\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# the list of gestures\n",
    "GESTURES = [\n",
    "    \"punch\",\n",
    "    \"flex\",\n",
    "]\n",
    "NUM_GESTURES = len(GESTURES)\n",
    "TOTAL_RECORDS_PER_BATCH = 119\n",
    "\n",
    "# create a one-hot encoded output matrix that is encoded to value 0 or 1\n",
    "ONE_HOT_ENCODED_GESTURES = np.eye(NUM_GESTURES)\n",
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "# read each csv file and push an input and output\n",
    "for gesture_index in range(NUM_GESTURES):\n",
    "    # get the current gesture (ex: punch or flex)\n",
    "    gesture = GESTURES[gesture_index]\n",
    "    print(f\"Processing index {gesture_index} for gesture '{gesture}'.\")\n",
    "\n",
    "    # assign output to the corresponding input index (the output will be encoded to 0 or 1)\n",
    "    output = ONE_HOT_ENCODED_GESTURES[gesture_index]\n",
    "\n",
    "    # read input from csv file\n",
    "    df = pd.read_csv(\"../include/\" + gesture + \".csv\")\n",
    "    print(df.head())\n",
    "    print(df.shape)\n",
    "    print(df.shape[0], \" -- \", TOTAL_RECORDS_PER_BATCH)\n",
    "\n",
    "    # partition input to multiple batches --> reason: faster execution\n",
    "    num_batches_per_gesture = int(df.shape[0] / TOTAL_RECORDS_PER_BATCH)\n",
    "\n",
    "    print(f\"\\tThere are {num_batches_per_gesture} batches of the *{gesture}* gesture.\\n\")\n",
    "\n",
    "    # go into each batch and start processing\n",
    "    for batch in range(num_batches_per_gesture):\n",
    "        tensor = []\n",
    "        for record in range(TOTAL_RECORDS_PER_BATCH):\n",
    "            index = batch * TOTAL_RECORDS_PER_BATCH + record\n",
    "            # normalize the input data, between 0 to 1:\n",
    "            # - acceleration is between: -2048 to +2048\n",
    "            tensor += [\n",
    "                (df[\"aX\"][index] + 2048.0) / 4096.0,\n",
    "                (df[\"aY\"][index] + 2048.0) / 4096.0,\n",
    "                (df[\"aZ\"][index] + 2048.0) / 4096.0,\n",
    "            ]\n",
    "        inputs.append(tensor)\n",
    "        outputs.append(output)\n",
    "\n",
    "# convert the list to numpy array\n",
    "inputs = np.array(inputs)\n",
    "outputs = np.array(outputs)\n",
    "\n",
    "print(\"Data set parsing and preparation complete.\")\n",
    "print(len(inputs), \"\\n\")\n",
    "print(\n",
    "    \"punch records length per batch = len([ax,ay,az]) * TOTAL_RECORDS_PER_BATCH = 3 * TOTAL_RECORDS_PER_BATCH = \",\n",
    "    len(inputs[0]),\n",
    "    \"\\n\",\n",
    ")\n",
    "print(\"punch input: \", inputs[0], \" output: \", outputs[0])\n",
    "print(\"flex input: \", inputs[1], \" output: \", outputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 357) (10, 2)\n",
      "(3, 357) (3, 2)\n",
      "(5, 357) (5, 2)\n",
      "Data set randomization and splitting complete.\n"
     ]
    }
   ],
   "source": [
    "# Randomize the order of the inputs, so they can be evenly distributed for training, testing, and validation\n",
    "# https://stackoverflow.com/a/37710486/2020087\n",
    "num_inputs = len(inputs)\n",
    "randomize = np.arange(num_inputs)\n",
    "np.random.shuffle(randomize)\n",
    "\n",
    "# Swap the consecutive indexes (0, 1, 2, etc) with the randomized indexes\n",
    "inputs = inputs[randomize]\n",
    "outputs = outputs[randomize]\n",
    "\n",
    "# Split the recordings (group of samples) into three sets: training, testing and validation\n",
    "TRAIN_SPLIT = int(0.6 * num_inputs)\n",
    "TEST_SPLIT = int(0.2 * num_inputs + TRAIN_SPLIT)\n",
    "\n",
    "inputs_train, inputs_test, inputs_val = np.split(inputs, [TRAIN_SPLIT, TEST_SPLIT])\n",
    "outputs_train, outputs_test, outputs_val = np.split(outputs, [TRAIN_SPLIT, TEST_SPLIT])\n",
    "\n",
    "print(inputs_train.shape, outputs_train.shape)\n",
    "print(inputs_test.shape, outputs_test.shape)\n",
    "print(inputs_val.shape, outputs_val.shape)\n",
    "\n",
    "print(\"Data set randomization and splitting complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.src.layers.core.dense.Dense at 0x1f44322ad90>,\n",
       " <keras.src.layers.core.dense.Dense at 0x1f44321a850>,\n",
       " <keras.src.layers.core.dense.Dense at 0x1f4432db5d0>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(50, activation=\"softplus\", input_shape=(357,)))  # relu is used for performance\n",
    "model.add(keras.layers.Dense(15, activation=\"softplus\"))\n",
    "model.add(keras.layers.Dense(NUM_GESTURES, activation=\"softmax\"))  # softmax is used, because we only expect one gesture to occur per input\n",
    "\n",
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(357,) (10, 357)\n",
      "Epoch 1/200\n",
      "10/10 [==============================] - 1s 23ms/step - loss: 0.4018 - mae: 0.5555 - val_loss: 0.2098 - val_mae: 0.3737\n",
      "Epoch 2/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.2305 - mae: 0.4559 - val_loss: 0.0830 - val_mae: 0.2773\n",
      "Epoch 3/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1117 - mae: 0.3030 - val_loss: 0.0414 - val_mae: 0.2008\n",
      "Epoch 4/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0570 - mae: 0.2155 - val_loss: 0.0407 - val_mae: 0.1838\n",
      "Epoch 5/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0327 - mae: 0.1610 - val_loss: 0.0101 - val_mae: 0.0960\n",
      "Epoch 6/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0192 - mae: 0.1162 - val_loss: 0.0101 - val_mae: 0.0900\n",
      "Epoch 7/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0094 - mae: 0.0853 - val_loss: 0.0113 - val_mae: 0.0872\n",
      "Epoch 8/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0068 - mae: 0.0696 - val_loss: 0.0038 - val_mae: 0.0531\n",
      "Epoch 9/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0020 - mae: 0.0410 - val_loss: 0.0016 - val_mae: 0.0329\n",
      "Epoch 10/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0021 - mae: 0.0377 - val_loss: 9.4671e-04 - val_mae: 0.0252\n",
      "Epoch 11/200\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0014 - mae: 0.0306 - val_loss: 5.7546e-04 - val_mae: 0.0211\n",
      "Epoch 12/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 7.3848e-04 - mae: 0.0232 - val_loss: 4.4441e-04 - val_mae: 0.0158\n",
      "Epoch 13/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 5.3875e-04 - mae: 0.0179 - val_loss: 2.3481e-04 - val_mae: 0.0127\n",
      "Epoch 14/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 3.0591e-04 - mae: 0.0149 - val_loss: 1.6155e-04 - val_mae: 0.0104\n",
      "Epoch 15/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 2.1550e-04 - mae: 0.0117 - val_loss: 1.5216e-04 - val_mae: 0.0101\n",
      "Epoch 16/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.3906e-04 - mae: 0.0098 - val_loss: 1.1368e-04 - val_mae: 0.0087\n",
      "Epoch 17/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.0207e-04 - mae: 0.0085 - val_loss: 7.5997e-05 - val_mae: 0.0071\n",
      "Epoch 18/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 7.2659e-05 - mae: 0.0072 - val_loss: 5.8102e-05 - val_mae: 0.0062\n",
      "Epoch 19/200\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 5.8966e-05 - mae: 0.0063 - val_loss: 7.0631e-05 - val_mae: 0.0065\n",
      "Epoch 20/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 5.0248e-05 - mae: 0.0058 - val_loss: 5.0567e-05 - val_mae: 0.0056\n",
      "Epoch 21/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 3.6762e-05 - mae: 0.0050 - val_loss: 5.3211e-05 - val_mae: 0.0056\n",
      "Epoch 22/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 3.3466e-05 - mae: 0.0048 - val_loss: 3.8593e-05 - val_mae: 0.0048\n",
      "Epoch 23/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 2.6395e-05 - mae: 0.0043 - val_loss: 2.8466e-05 - val_mae: 0.0042\n",
      "Epoch 24/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 2.2327e-05 - mae: 0.0040 - val_loss: 2.4466e-05 - val_mae: 0.0039\n",
      "Epoch 25/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.9715e-05 - mae: 0.0037 - val_loss: 2.2662e-05 - val_mae: 0.0037\n",
      "Epoch 26/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.6558e-05 - mae: 0.0033 - val_loss: 2.4211e-05 - val_mae: 0.0037\n",
      "Epoch 27/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.5124e-05 - mae: 0.0032 - val_loss: 2.2301e-05 - val_mae: 0.0036\n",
      "Epoch 28/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.3473e-05 - mae: 0.0030 - val_loss: 1.7984e-05 - val_mae: 0.0032\n",
      "Epoch 29/200\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1.2028e-05 - mae: 0.0028 - val_loss: 1.5798e-05 - val_mae: 0.0030\n",
      "Epoch 30/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.0977e-05 - mae: 0.0027 - val_loss: 1.5351e-05 - val_mae: 0.0030\n",
      "Epoch 31/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.0010e-05 - mae: 0.0026 - val_loss: 1.4536e-05 - val_mae: 0.0029\n",
      "Epoch 32/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 9.1993e-06 - mae: 0.0025 - val_loss: 1.3700e-05 - val_mae: 0.0028\n",
      "Epoch 33/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 8.4394e-06 - mae: 0.0024 - val_loss: 1.3223e-05 - val_mae: 0.0027\n",
      "Epoch 34/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 7.8722e-06 - mae: 0.0023 - val_loss: 1.2454e-05 - val_mae: 0.0026\n",
      "Epoch 35/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 7.3249e-06 - mae: 0.0022 - val_loss: 1.1887e-05 - val_mae: 0.0026\n",
      "Epoch 36/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 6.8593e-06 - mae: 0.0021 - val_loss: 1.1314e-05 - val_mae: 0.0025\n",
      "Epoch 37/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 6.4775e-06 - mae: 0.0021 - val_loss: 1.0542e-05 - val_mae: 0.0024\n",
      "Epoch 38/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 6.0736e-06 - mae: 0.0020 - val_loss: 1.0028e-05 - val_mae: 0.0023\n",
      "Epoch 39/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 5.7436e-06 - mae: 0.0020 - val_loss: 9.5499e-06 - val_mae: 0.0023\n",
      "Epoch 40/200\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 5.4475e-06 - mae: 0.0019 - val_loss: 9.0066e-06 - val_mae: 0.0022\n",
      "Epoch 41/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 5.1615e-06 - mae: 0.0019 - val_loss: 8.6242e-06 - val_mae: 0.0022\n",
      "Epoch 42/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 4.9083e-06 - mae: 0.0018 - val_loss: 8.0421e-06 - val_mae: 0.0021\n",
      "Epoch 43/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 4.6805e-06 - mae: 0.0018 - val_loss: 7.6012e-06 - val_mae: 0.0020\n",
      "Epoch 44/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 4.4800e-06 - mae: 0.0017 - val_loss: 7.3203e-06 - val_mae: 0.0020\n",
      "Epoch 45/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 4.2946e-06 - mae: 0.0017 - val_loss: 7.0557e-06 - val_mae: 0.0020\n",
      "Epoch 46/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 4.1110e-06 - mae: 0.0016 - val_loss: 6.8811e-06 - val_mae: 0.0019\n",
      "Epoch 47/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 3.9530e-06 - mae: 0.0016 - val_loss: 6.5778e-06 - val_mae: 0.0019\n",
      "Epoch 48/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 3.7952e-06 - mae: 0.0016 - val_loss: 6.4380e-06 - val_mae: 0.0019\n",
      "Epoch 49/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3.6661e-06 - mae: 0.0015 - val_loss: 6.2198e-06 - val_mae: 0.0018\n",
      "Epoch 50/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3.5339e-06 - mae: 0.0015 - val_loss: 6.0007e-06 - val_mae: 0.0018\n",
      "Epoch 51/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 3.4103e-06 - mae: 0.0015 - val_loss: 5.8433e-06 - val_mae: 0.0018\n",
      "Epoch 52/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 3.2955e-06 - mae: 0.0015 - val_loss: 5.6886e-06 - val_mae: 0.0017\n",
      "Epoch 53/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 3.1912e-06 - mae: 0.0014 - val_loss: 5.5317e-06 - val_mae: 0.0017\n",
      "Epoch 54/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 3.0900e-06 - mae: 0.0014 - val_loss: 5.3445e-06 - val_mae: 0.0017\n",
      "Epoch 55/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 2.9951e-06 - mae: 0.0014 - val_loss: 5.2027e-06 - val_mae: 0.0017\n",
      "Epoch 56/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2.9072e-06 - mae: 0.0014 - val_loss: 5.0569e-06 - val_mae: 0.0016\n",
      "Epoch 57/200\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 2.8212e-06 - mae: 0.0014 - val_loss: 4.8988e-06 - val_mae: 0.0016\n",
      "Epoch 58/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2.7434e-06 - mae: 0.0013 - val_loss: 4.7834e-06 - val_mae: 0.0016\n",
      "Epoch 59/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2.6679e-06 - mae: 0.0013 - val_loss: 4.6696e-06 - val_mae: 0.0016\n",
      "Epoch 60/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 2.5960e-06 - mae: 0.0013 - val_loss: 4.5653e-06 - val_mae: 0.0016\n",
      "Epoch 61/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 2.5274e-06 - mae: 0.0013 - val_loss: 4.4503e-06 - val_mae: 0.0015\n",
      "Epoch 62/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 2.4609e-06 - mae: 0.0013 - val_loss: 4.3740e-06 - val_mae: 0.0015\n",
      "Epoch 63/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2.4016e-06 - mae: 0.0013 - val_loss: 4.2848e-06 - val_mae: 0.0015\n",
      "Epoch 64/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 2.3426e-06 - mae: 0.0012 - val_loss: 4.1754e-06 - val_mae: 0.0015\n",
      "Epoch 65/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 2.2872e-06 - mae: 0.0012 - val_loss: 4.0931e-06 - val_mae: 0.0015\n",
      "Epoch 66/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 2.2316e-06 - mae: 0.0012 - val_loss: 4.0262e-06 - val_mae: 0.0015\n",
      "Epoch 67/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 2.1821e-06 - mae: 0.0012 - val_loss: 3.9268e-06 - val_mae: 0.0014\n",
      "Epoch 68/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 2.1316e-06 - mae: 0.0012 - val_loss: 3.8689e-06 - val_mae: 0.0014\n",
      "Epoch 69/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 2.0855e-06 - mae: 0.0012 - val_loss: 3.8085e-06 - val_mae: 0.0014\n",
      "Epoch 70/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 2.0413e-06 - mae: 0.0012 - val_loss: 3.7432e-06 - val_mae: 0.0014\n",
      "Epoch 71/200\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1.9977e-06 - mae: 0.0011 - val_loss: 3.6833e-06 - val_mae: 0.0014\n",
      "Epoch 72/200\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1.9576e-06 - mae: 0.0011 - val_loss: 3.6201e-06 - val_mae: 0.0014\n",
      "Epoch 73/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.9185e-06 - mae: 0.0011 - val_loss: 3.5532e-06 - val_mae: 0.0014\n",
      "Epoch 74/200\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1.8799e-06 - mae: 0.0011 - val_loss: 3.4760e-06 - val_mae: 0.0013\n",
      "Epoch 75/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.8434e-06 - mae: 0.0011 - val_loss: 3.4052e-06 - val_mae: 0.0013\n",
      "Epoch 76/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.8067e-06 - mae: 0.0011 - val_loss: 3.3565e-06 - val_mae: 0.0013\n",
      "Epoch 77/200\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1.7738e-06 - mae: 0.0011 - val_loss: 3.2888e-06 - val_mae: 0.0013\n",
      "Epoch 78/200\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1.7410e-06 - mae: 0.0011 - val_loss: 3.2342e-06 - val_mae: 0.0013\n",
      "Epoch 79/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.7089e-06 - mae: 0.0011 - val_loss: 3.1841e-06 - val_mae: 0.0013\n",
      "Epoch 80/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.6785e-06 - mae: 0.0010 - val_loss: 3.1258e-06 - val_mae: 0.0013\n",
      "Epoch 81/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.6481e-06 - mae: 0.0010 - val_loss: 3.0841e-06 - val_mae: 0.0013\n",
      "Epoch 82/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.6195e-06 - mae: 0.0010 - val_loss: 3.0432e-06 - val_mae: 0.0013\n",
      "Epoch 83/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.5923e-06 - mae: 0.0010 - val_loss: 3.0003e-06 - val_mae: 0.0012\n",
      "Epoch 84/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.5661e-06 - mae: 0.0010 - val_loss: 2.9496e-06 - val_mae: 0.0012\n",
      "Epoch 85/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.5399e-06 - mae: 9.9543e-04 - val_loss: 2.9036e-06 - val_mae: 0.0012\n",
      "Epoch 86/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.5148e-06 - mae: 9.8597e-04 - val_loss: 2.8548e-06 - val_mae: 0.0012\n",
      "Epoch 87/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.4903e-06 - mae: 9.7842e-04 - val_loss: 2.8077e-06 - val_mae: 0.0012\n",
      "Epoch 88/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.4669e-06 - mae: 9.6972e-04 - val_loss: 2.7696e-06 - val_mae: 0.0012\n",
      "Epoch 89/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.4438e-06 - mae: 9.6103e-04 - val_loss: 2.7253e-06 - val_mae: 0.0012\n",
      "Epoch 90/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.4219e-06 - mae: 9.5471e-04 - val_loss: 2.6871e-06 - val_mae: 0.0012\n",
      "Epoch 91/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.4001e-06 - mae: 9.4547e-04 - val_loss: 2.6458e-06 - val_mae: 0.0012\n",
      "Epoch 92/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.3790e-06 - mae: 9.4068e-04 - val_loss: 2.6065e-06 - val_mae: 0.0012\n",
      "Epoch 93/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.3587e-06 - mae: 9.3214e-04 - val_loss: 2.5765e-06 - val_mae: 0.0012\n",
      "Epoch 94/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.3392e-06 - mae: 9.2519e-04 - val_loss: 2.5405e-06 - val_mae: 0.0011\n",
      "Epoch 95/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.3199e-06 - mae: 9.1696e-04 - val_loss: 2.5054e-06 - val_mae: 0.0011\n",
      "Epoch 96/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.3012e-06 - mae: 9.1260e-04 - val_loss: 2.4709e-06 - val_mae: 0.0011\n",
      "Epoch 97/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.2832e-06 - mae: 9.0505e-04 - val_loss: 2.4410e-06 - val_mae: 0.0011\n",
      "Epoch 98/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.2654e-06 - mae: 8.9927e-04 - val_loss: 2.4115e-06 - val_mae: 0.0011\n",
      "Epoch 99/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.2479e-06 - mae: 8.9065e-04 - val_loss: 2.3866e-06 - val_mae: 0.0011\n",
      "Epoch 100/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.2314e-06 - mae: 8.8721e-04 - val_loss: 2.3564e-06 - val_mae: 0.0011\n",
      "Epoch 101/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.2147e-06 - mae: 8.7995e-04 - val_loss: 2.3323e-06 - val_mae: 0.0011\n",
      "Epoch 102/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.1990e-06 - mae: 8.7385e-04 - val_loss: 2.3024e-06 - val_mae: 0.0011\n",
      "Epoch 103/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.1833e-06 - mae: 8.6832e-04 - val_loss: 2.2727e-06 - val_mae: 0.0011\n",
      "Epoch 104/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.1681e-06 - mae: 8.6246e-04 - val_loss: 2.2503e-06 - val_mae: 0.0011\n",
      "Epoch 105/200\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1.1535e-06 - mae: 8.5745e-04 - val_loss: 2.2232e-06 - val_mae: 0.0011\n",
      "Epoch 106/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.1389e-06 - mae: 8.5214e-04 - val_loss: 2.2007e-06 - val_mae: 0.0011\n",
      "Epoch 107/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.1249e-06 - mae: 8.4643e-04 - val_loss: 2.1746e-06 - val_mae: 0.0011\n",
      "Epoch 108/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.1112e-06 - mae: 8.4128e-04 - val_loss: 2.1502e-06 - val_mae: 0.0010\n",
      "Epoch 109/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.0976e-06 - mae: 8.3479e-04 - val_loss: 2.1295e-06 - val_mae: 0.0010\n",
      "Epoch 110/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.0845e-06 - mae: 8.2947e-04 - val_loss: 2.1091e-06 - val_mae: 0.0010\n",
      "Epoch 111/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.0716e-06 - mae: 8.2601e-04 - val_loss: 2.0884e-06 - val_mae: 0.0010\n",
      "Epoch 112/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.0592e-06 - mae: 8.2132e-04 - val_loss: 2.0657e-06 - val_mae: 0.0010\n",
      "Epoch 113/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.0468e-06 - mae: 8.1582e-04 - val_loss: 2.0458e-06 - val_mae: 0.0010\n",
      "Epoch 114/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.0350e-06 - mae: 8.1091e-04 - val_loss: 2.0244e-06 - val_mae: 0.0010\n",
      "Epoch 115/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.0231e-06 - mae: 8.0652e-04 - val_loss: 2.0052e-06 - val_mae: 0.0010\n",
      "Epoch 116/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.0118e-06 - mae: 8.0139e-04 - val_loss: 1.9841e-06 - val_mae: 0.0010\n",
      "Epoch 117/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.0004e-06 - mae: 7.9723e-04 - val_loss: 1.9620e-06 - val_mae: 9.9989e-04\n",
      "Epoch 118/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 9.8944e-07 - mae: 7.9332e-04 - val_loss: 1.9426e-06 - val_mae: 9.9467e-04\n",
      "Epoch 119/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 9.7871e-07 - mae: 7.8795e-04 - val_loss: 1.9237e-06 - val_mae: 9.8956e-04\n",
      "Epoch 120/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 9.6817e-07 - mae: 7.8394e-04 - val_loss: 1.9038e-06 - val_mae: 9.8430e-04\n",
      "Epoch 121/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 9.5781e-07 - mae: 7.7906e-04 - val_loss: 1.8841e-06 - val_mae: 9.7904e-04\n",
      "Epoch 122/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 9.4776e-07 - mae: 7.7503e-04 - val_loss: 1.8666e-06 - val_mae: 9.7421e-04\n",
      "Epoch 123/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 9.3782e-07 - mae: 7.7087e-04 - val_loss: 1.8478e-06 - val_mae: 9.6917e-04\n",
      "Epoch 124/200\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 9.2796e-07 - mae: 7.6669e-04 - val_loss: 1.8323e-06 - val_mae: 9.6472e-04\n",
      "Epoch 125/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 9.1860e-07 - mae: 7.6254e-04 - val_loss: 1.8157e-06 - val_mae: 9.6013e-04\n",
      "Epoch 126/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 9.0915e-07 - mae: 7.5925e-04 - val_loss: 1.7976e-06 - val_mae: 9.5520e-04\n",
      "Epoch 127/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 9.0003e-07 - mae: 7.5486e-04 - val_loss: 1.7800e-06 - val_mae: 9.5039e-04\n",
      "Epoch 128/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 8.9107e-07 - mae: 7.5084e-04 - val_loss: 1.7648e-06 - val_mae: 9.4606e-04\n",
      "Epoch 129/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 8.8230e-07 - mae: 7.4553e-04 - val_loss: 1.7484e-06 - val_mae: 9.4149e-04\n",
      "Epoch 130/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 8.7370e-07 - mae: 7.4307e-04 - val_loss: 1.7332e-06 - val_mae: 9.3718e-04\n",
      "Epoch 131/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 8.6501e-07 - mae: 7.3994e-04 - val_loss: 1.7197e-06 - val_mae: 9.3319e-04\n",
      "Epoch 132/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 8.5686e-07 - mae: 7.3603e-04 - val_loss: 1.7045e-06 - val_mae: 9.2887e-04\n",
      "Epoch 133/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 8.4867e-07 - mae: 7.3158e-04 - val_loss: 1.6889e-06 - val_mae: 9.2447e-04\n",
      "Epoch 134/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 8.4057e-07 - mae: 7.2856e-04 - val_loss: 1.6756e-06 - val_mae: 9.2057e-04\n",
      "Epoch 135/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 8.3273e-07 - mae: 7.2561e-04 - val_loss: 1.6602e-06 - val_mae: 9.1622e-04\n",
      "Epoch 136/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 8.2504e-07 - mae: 7.2183e-04 - val_loss: 1.6468e-06 - val_mae: 9.1230e-04\n",
      "Epoch 137/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 8.1744e-07 - mae: 7.1835e-04 - val_loss: 1.6334e-06 - val_mae: 9.0836e-04\n",
      "Epoch 138/200\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 8.0996e-07 - mae: 7.1499e-04 - val_loss: 1.6197e-06 - val_mae: 9.0437e-04\n",
      "Epoch 139/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 8.0267e-07 - mae: 7.1152e-04 - val_loss: 1.6064e-06 - val_mae: 9.0049e-04\n",
      "Epoch 140/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 7.9548e-07 - mae: 7.0842e-04 - val_loss: 1.5934e-06 - val_mae: 8.9665e-04\n",
      "Epoch 141/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 7.8824e-07 - mae: 7.0592e-04 - val_loss: 1.5818e-06 - val_mae: 8.9311e-04\n",
      "Epoch 142/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 7.8137e-07 - mae: 7.0234e-04 - val_loss: 1.5691e-06 - val_mae: 8.8934e-04\n",
      "Epoch 143/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 7.7441e-07 - mae: 6.9883e-04 - val_loss: 1.5578e-06 - val_mae: 8.8589e-04\n",
      "Epoch 144/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 7.6781e-07 - mae: 6.9535e-04 - val_loss: 1.5448e-06 - val_mae: 8.8207e-04\n",
      "Epoch 145/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 7.6111e-07 - mae: 6.9235e-04 - val_loss: 1.5334e-06 - val_mae: 8.7860e-04\n",
      "Epoch 146/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 7.5461e-07 - mae: 6.8928e-04 - val_loss: 1.5225e-06 - val_mae: 8.7522e-04\n",
      "Epoch 147/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 7.4826e-07 - mae: 6.8600e-04 - val_loss: 1.5099e-06 - val_mae: 8.7151e-04\n",
      "Epoch 148/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 7.4197e-07 - mae: 6.8351e-04 - val_loss: 1.4984e-06 - val_mae: 8.6802e-04\n",
      "Epoch 149/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 7.3579e-07 - mae: 6.8053e-04 - val_loss: 1.4871e-06 - val_mae: 8.6458e-04\n",
      "Epoch 150/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 7.2967e-07 - mae: 6.7835e-04 - val_loss: 1.4760e-06 - val_mae: 8.6118e-04\n",
      "Epoch 151/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 7.2365e-07 - mae: 6.7462e-04 - val_loss: 1.4643e-06 - val_mae: 8.5766e-04\n",
      "Epoch 152/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 7.1776e-07 - mae: 6.7197e-04 - val_loss: 1.4536e-06 - val_mae: 8.5438e-04\n",
      "Epoch 153/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 7.1192e-07 - mae: 6.6900e-04 - val_loss: 1.4434e-06 - val_mae: 8.5119e-04\n",
      "Epoch 154/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 7.0619e-07 - mae: 6.6630e-04 - val_loss: 1.4330e-06 - val_mae: 8.4795e-04\n",
      "Epoch 155/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 7.0057e-07 - mae: 6.6349e-04 - val_loss: 1.4229e-06 - val_mae: 8.4479e-04\n",
      "Epoch 156/200\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 6.9498e-07 - mae: 6.6063e-04 - val_loss: 1.4133e-06 - val_mae: 8.4175e-04\n",
      "Epoch 157/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 6.8956e-07 - mae: 6.5761e-04 - val_loss: 1.4028e-06 - val_mae: 8.3850e-04\n",
      "Epoch 158/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 6.8416e-07 - mae: 6.5524e-04 - val_loss: 1.3932e-06 - val_mae: 8.3545e-04\n",
      "Epoch 159/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 6.7877e-07 - mae: 6.5317e-04 - val_loss: 1.3840e-06 - val_mae: 8.3251e-04\n",
      "Epoch 160/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 6.7357e-07 - mae: 6.4998e-04 - val_loss: 1.3751e-06 - val_mae: 8.2961e-04\n",
      "Epoch 161/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 6.6845e-07 - mae: 6.4759e-04 - val_loss: 1.3659e-06 - val_mae: 8.2670e-04\n",
      "Epoch 162/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 6.6344e-07 - mae: 6.4510e-04 - val_loss: 1.3563e-06 - val_mae: 8.2366e-04\n",
      "Epoch 163/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 6.5842e-07 - mae: 6.4284e-04 - val_loss: 1.3469e-06 - val_mae: 8.2067e-04\n",
      "Epoch 164/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 6.5342e-07 - mae: 6.4032e-04 - val_loss: 1.3383e-06 - val_mae: 8.1788e-04\n",
      "Epoch 165/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 6.4867e-07 - mae: 6.3730e-04 - val_loss: 1.3289e-06 - val_mae: 8.1489e-04\n",
      "Epoch 166/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 6.4384e-07 - mae: 6.3496e-04 - val_loss: 1.3194e-06 - val_mae: 8.1190e-04\n",
      "Epoch 167/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 6.3913e-07 - mae: 6.3248e-04 - val_loss: 1.3105e-06 - val_mae: 8.0905e-04\n",
      "Epoch 168/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 6.3447e-07 - mae: 6.3078e-04 - val_loss: 1.3018e-06 - val_mae: 8.0624e-04\n",
      "Epoch 169/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 6.2988e-07 - mae: 6.2795e-04 - val_loss: 1.2927e-06 - val_mae: 8.0333e-04\n",
      "Epoch 170/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 6.2535e-07 - mae: 6.2556e-04 - val_loss: 1.2842e-06 - val_mae: 8.0056e-04\n",
      "Epoch 171/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 6.2088e-07 - mae: 6.2364e-04 - val_loss: 1.2754e-06 - val_mae: 7.9775e-04\n",
      "Epoch 172/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 6.1648e-07 - mae: 6.2158e-04 - val_loss: 1.2673e-06 - val_mae: 7.9508e-04\n",
      "Epoch 173/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 6.1212e-07 - mae: 6.1928e-04 - val_loss: 1.2592e-06 - val_mae: 7.9242e-04\n",
      "Epoch 174/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 6.0785e-07 - mae: 6.1618e-04 - val_loss: 1.2512e-06 - val_mae: 7.8978e-04\n",
      "Epoch 175/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 6.0363e-07 - mae: 6.1445e-04 - val_loss: 1.2431e-06 - val_mae: 7.8711e-04\n",
      "Epoch 176/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 5.9942e-07 - mae: 6.1219e-04 - val_loss: 1.2348e-06 - val_mae: 7.8440e-04\n",
      "Epoch 177/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 5.9532e-07 - mae: 6.0984e-04 - val_loss: 1.2266e-06 - val_mae: 7.8174e-04\n",
      "Epoch 178/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 5.9125e-07 - mae: 6.0832e-04 - val_loss: 1.2185e-06 - val_mae: 7.7907e-04\n",
      "Epoch 179/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 5.8728e-07 - mae: 6.0573e-04 - val_loss: 1.2110e-06 - val_mae: 7.7654e-04\n",
      "Epoch 180/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 5.8332e-07 - mae: 6.0369e-04 - val_loss: 1.2033e-06 - val_mae: 7.7400e-04\n",
      "Epoch 181/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 5.7942e-07 - mae: 6.0161e-04 - val_loss: 1.1960e-06 - val_mae: 7.7154e-04\n",
      "Epoch 182/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 5.7556e-07 - mae: 5.9940e-04 - val_loss: 1.1888e-06 - val_mae: 7.6909e-04\n",
      "Epoch 183/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 5.7175e-07 - mae: 5.9723e-04 - val_loss: 1.1816e-06 - val_mae: 7.6667e-04\n",
      "Epoch 184/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 5.6799e-07 - mae: 5.9557e-04 - val_loss: 1.1743e-06 - val_mae: 7.6421e-04\n",
      "Epoch 185/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 5.6422e-07 - mae: 5.9350e-04 - val_loss: 1.1678e-06 - val_mae: 7.6195e-04\n",
      "Epoch 186/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 5.6062e-07 - mae: 5.9171e-04 - val_loss: 1.1610e-06 - val_mae: 7.5961e-04\n",
      "Epoch 187/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 5.5696e-07 - mae: 5.8962e-04 - val_loss: 1.1546e-06 - val_mae: 7.5739e-04\n",
      "Epoch 188/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 5.5343e-07 - mae: 5.8750e-04 - val_loss: 1.1478e-06 - val_mae: 7.5505e-04\n",
      "Epoch 189/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 5.4988e-07 - mae: 5.8577e-04 - val_loss: 1.1408e-06 - val_mae: 7.5268e-04\n",
      "Epoch 190/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 5.4636e-07 - mae: 5.8366e-04 - val_loss: 1.1347e-06 - val_mae: 7.5051e-04\n",
      "Epoch 191/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 5.4295e-07 - mae: 5.8201e-04 - val_loss: 1.1281e-06 - val_mae: 7.4826e-04\n",
      "Epoch 192/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 5.3953e-07 - mae: 5.8002e-04 - val_loss: 1.1214e-06 - val_mae: 7.4592e-04\n",
      "Epoch 193/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 5.3618e-07 - mae: 5.7814e-04 - val_loss: 1.1153e-06 - val_mae: 7.4378e-04\n",
      "Epoch 194/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 5.3285e-07 - mae: 5.7582e-04 - val_loss: 1.1087e-06 - val_mae: 7.4153e-04\n",
      "Epoch 195/200\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 5.2958e-07 - mae: 5.7397e-04 - val_loss: 1.1023e-06 - val_mae: 7.3929e-04\n",
      "Epoch 196/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 5.2633e-07 - mae: 5.7278e-04 - val_loss: 1.0961e-06 - val_mae: 7.3713e-04\n",
      "Epoch 197/200\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 5.2314e-07 - mae: 5.7076e-04 - val_loss: 1.0903e-06 - val_mae: 7.3504e-04\n",
      "Epoch 198/200\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 5.1996e-07 - mae: 5.6903e-04 - val_loss: 1.0840e-06 - val_mae: 7.3287e-04\n",
      "Epoch 199/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 5.1683e-07 - mae: 5.6763e-04 - val_loss: 1.0778e-06 - val_mae: 7.3070e-04\n",
      "Epoch 200/200\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 5.1370e-07 - mae: 5.6547e-04 - val_loss: 1.0724e-06 - val_mae: 7.2872e-04\n"
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "print(inputs_train[0].shape, inputs_train.shape)\n",
    "\n",
    "history = model.fit(\n",
    "    inputs_train,\n",
    "    outputs_train,\n",
    "    epochs=200,\n",
    "    batch_size=1,\n",
    "    validation_data=(inputs_val, outputs_val),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_11 (Dense)            (None, 50)                17900     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 15)                765       \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 2)                 32        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18697 (73.04 KB)\n",
      "Trainable params: 18697 (73.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(357,)\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "predictions =\n",
      " [[0.714 0.286]\n",
      " [0.    1.   ]\n",
      " [1.    0.   ]]\n",
      "actual =\n",
      " [[1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "10   3   5   3\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# use the model to predict the test inputs\n",
    "print(inputs_test[0].shape)\n",
    "predictions = model.predict(inputs_test)\n",
    "# print the predictions and the expected ouputs\n",
    "print(\"predictions =\\n\", np.round(predictions, decimals=3))\n",
    "print(\"actual =\\n\", outputs_test)\n",
    "print(\n",
    "    len(inputs_train),\n",
    "    \" \",\n",
    "    len(inputs_test),\n",
    "    \" \",\n",
    "    len(inputs_val),\n",
    "    \" \",\n",
    "    len(predictions),\n",
    ")\n",
    "\n",
    "\n",
    "# Convert predictions to class labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "actual_labels = np.argmax(outputs_test, axis=1)\n",
    "\n",
    "# Compare predictions with actual labels\n",
    "correct_predictions = np.sum(predicted_labels == actual_labels)\n",
    "total_predictions = len(actual_labels)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_dataset():\n",
    "    for data in tf.data.Dataset.from_tensor_slices((inputs)).batch(1).take(100):\n",
    "        yield [tf.dtypes.cast(data, tf.float32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\quoct\\AppData\\Local\\Temp\\tmplwp1q338\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\quoct\\AppData\\Local\\Temp\\tmplwp1q338\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is 23432 bytes\n"
     ]
    }
   ],
   "source": [
    "# Convert the model to the TensorFlow Lite format without quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# converter.representative_dataset = representative_dataset\n",
    "# # converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "# converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "# converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "# converter.inference_input_type = tf.float32  # or tf.uint8\n",
    "# converter.inference_output_type = tf.float32  # or tf.uint8\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model to disk\n",
    "open(\"gesture_model.tflite\", \"wb\").write(tflite_model)\n",
    "\n",
    "import os\n",
    "\n",
    "basic_model_size = os.path.getsize(\"gesture_model.tflite\")\n",
    "print(\"Model is %d bytes\" % basic_model_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_details: [{'name': 'serving_default_dense_input:0', 'index': 0, 'shape': array([  1, 357]), 'shape_signature': array([ -1, 357]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "output_details: [{'name': 'StatefulPartitionedCall:0', 'index': 10, 'shape': array([1, 2]), 'shape_signature': array([-1,  2]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "[[0.27539062 0.65893555 0.25683594 0.27539062 0.65893555 0.25683594\n",
      "  0.27539062 0.65893555 0.25683594 0.27539062 0.65893555 0.25683594\n",
      "  0.3684082  0.14794922 0.17041016 0.3684082  0.14794922 0.17041016\n",
      "  0.3684082  0.14794922 0.17041016 0.3684082  0.14794922 0.17041016\n",
      "  0.2998047  0.03588867 0.19848633 0.2998047  0.03588867 0.19848633\n",
      "  0.2998047  0.03588867 0.19848633 0.2998047  0.03588867 0.19848633\n",
      "  0.2998047  0.03588867 0.19848633 0.2998047  0.03588867 0.19848633\n",
      "  0.2998047  0.03588867 0.19848633 0.14355469 0.04516602 0.35351562\n",
      "  0.14355469 0.04516602 0.35351562 0.14355469 0.04516602 0.35351562\n",
      "  0.14355469 0.04516602 0.35351562 0.14355469 0.04516602 0.35351562\n",
      "  0.14355469 0.04516602 0.35351562 0.14355469 0.04516602 0.35351562\n",
      "  0.14355469 0.04516602 0.35351562 0.14355469 0.04516602 0.35351562\n",
      "  0.14355469 0.04516602 0.35351562 0.         0.17211914 0.4501953\n",
      "  0.         0.17211914 0.4501953  0.         0.17211914 0.4501953\n",
      "  0.         0.17211914 0.4501953  0.         0.17211914 0.4501953\n",
      "  0.         0.17211914 0.4501953  0.         0.17211914 0.4501953\n",
      "  0.         0.17211914 0.4501953  0.         0.17211914 0.4501953\n",
      "  0.         0.17211914 0.4501953  0.26733398 0.4885254  0.2277832\n",
      "  0.26733398 0.4885254  0.2277832  0.26733398 0.4885254  0.2277832\n",
      "  0.26733398 0.4885254  0.2277832  0.26733398 0.4885254  0.2277832\n",
      "  0.26733398 0.4885254  0.2277832  0.26733398 0.4885254  0.2277832\n",
      "  0.26733398 0.4885254  0.2277832  0.64331055 0.6335449  0.06958008\n",
      "  0.64331055 0.6335449  0.06958008 0.64331055 0.6335449  0.06958008\n",
      "  0.64331055 0.6335449  0.06958008 0.64331055 0.6335449  0.06958008\n",
      "  0.72924805 0.7043457  0.         0.72924805 0.7043457  0.\n",
      "  0.72924805 0.7043457  0.         0.72924805 0.7043457  0.\n",
      "  0.72924805 0.7043457  0.         0.72924805 0.7043457  0.\n",
      "  0.72924805 0.7043457  0.         0.72924805 0.7043457  0.\n",
      "  0.72924805 0.7043457  0.         0.72924805 0.7043457  0.\n",
      "  0.72924805 0.7043457  0.         0.72924805 0.7043457  0.\n",
      "  0.72924805 0.7043457  0.         0.68359375 0.78393555 0.04272461\n",
      "  0.61572266 0.8852539  0.22680664 0.61572266 0.8852539  0.22680664\n",
      "  0.61572266 0.8852539  0.22680664 0.61572266 0.8852539  0.22680664\n",
      "  0.61572266 0.8852539  0.22680664 0.61572266 0.8852539  0.22680664\n",
      "  0.61572266 0.8852539  0.22680664 0.61572266 0.8852539  0.22680664\n",
      "  0.61572266 0.8852539  0.22680664 0.61572266 0.8852539  0.22680664\n",
      "  0.61572266 0.8852539  0.22680664 0.63183594 0.9914551  0.1430664\n",
      "  0.63183594 0.9914551  0.1430664  0.63183594 0.9914551  0.1430664\n",
      "  0.63183594 0.9914551  0.1430664  0.63183594 0.9914551  0.1430664\n",
      "  0.63183594 0.9914551  0.1430664  0.63183594 0.9914551  0.1430664\n",
      "  0.63183594 0.9914551  0.1430664  0.9296875  0.99975586 0.\n",
      "  0.9296875  0.99975586 0.         0.9296875  0.99975586 0.\n",
      "  0.99975586 0.8117676  0.35107422 0.99975586 0.8117676  0.35107422\n",
      "  0.99975586 0.8117676  0.35107422 0.99975586 0.8117676  0.35107422\n",
      "  0.99975586 0.8117676  0.35107422 0.99975586 0.8117676  0.35107422\n",
      "  0.99975586 0.8117676  0.35107422 0.99975586 0.8117676  0.35107422\n",
      "  0.99975586 0.8117676  0.35107422 0.99975586 0.8117676  0.35107422\n",
      "  0.99975586 0.8117676  0.35107422 0.99975586 0.8117676  0.35107422\n",
      "  0.9458008  0.2836914  0.6779785  0.9458008  0.2836914  0.6779785\n",
      "  0.9458008  0.2836914  0.6779785  0.6381836  0.1953125  0.44311523\n",
      "  0.6381836  0.1953125  0.44311523 0.6381836  0.1953125  0.44311523\n",
      "  0.6381836  0.1953125  0.44311523 0.6381836  0.1953125  0.44311523\n",
      "  0.6381836  0.1953125  0.44311523 0.6381836  0.1953125  0.44311523\n",
      "  0.6381836  0.1953125  0.44311523 0.6381836  0.1953125  0.44311523\n",
      "  0.43920898 0.59765625 0.31298828 0.43920898 0.59765625 0.31298828\n",
      "  0.43920898 0.59765625 0.31298828 0.43920898 0.59765625 0.31298828\n",
      "  0.43920898 0.59765625 0.31298828 0.43920898 0.59765625 0.31298828\n",
      "  0.43920898 0.59765625 0.31298828 0.43920898 0.59765625 0.31298828\n",
      "  0.43920898 0.59765625 0.31298828 0.43920898 0.59765625 0.31298828\n",
      "  0.34277344 0.71777344 0.22924805]]\n",
      "[[0.87421775 0.12578222]]\n",
      "(1, 2)\n"
     ]
    }
   ],
   "source": [
    "# Get Interpreter to execute model\n",
    "interpreter = tf.lite.Interpreter(model_path=\"gesture_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(f\"input_details: {input_details}\")\n",
    "print(f\"output_details: {output_details}\")\n",
    "\n",
    "input_data = inputs_test[0:1].astype(np.float32)\n",
    "\n",
    "print(input_data)\n",
    "# push input data to interpreter\n",
    "interpreter.set_tensor(input_details[0][\"index\"], input_data)\n",
    "\n",
    "# start execution\n",
    "interpreter.invoke()\n",
    "\n",
    "# get the output of this model\n",
    "output_data = interpreter.get_tensor(output_details[0][\"index\"])\n",
    "\n",
    "print(output_data)\n",
    "print(output_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tflite_to_header(tflite_path, output_header_path):\n",
    "    with open(tflite_path, \"rb\") as tflite_file:\n",
    "        tflite_content = tflite_file.read()\n",
    "\n",
    "    hex_lines = [\", \".join([f\"0x{byte:02x}\" for byte in tflite_content[i : i + 12]]) for i in range(0, len(tflite_content), 12)]\n",
    "\n",
    "    hex_array = \",\\n  \".join(hex_lines)\n",
    "\n",
    "    with open(output_header_path, \"w\") as header_file:\n",
    "        header_file.write(\"alignas(8) const unsigned char g_model[] = {\\n  \")\n",
    "        header_file.write(f\"{hex_array}\\n\")\n",
    "        header_file.write(\"};\\n\\n\")\n",
    "\n",
    "\n",
    "tflite_path = \"gesture_model.tflite\"\n",
    "output_header_path = \"../include/g_model.h\"\n",
    "\n",
    "convert_tflite_to_header(tflite_path, output_header_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Convert some hex value into an array for C programming\n",
    "def hex_to_c_array(hex_data, var_name):\n",
    "    c_str = \"\"\n",
    "\n",
    "    # Create header guard\n",
    "    c_str += \"#ifndef \" + var_name.upper() + \"_H\\n\"\n",
    "    c_str += \"#define \" + var_name.upper() + \"_H\\n\\n\"\n",
    "\n",
    "    # Add array length at top of file\n",
    "    c_str += \"\\nunsigned int \" + var_name + \"_len = \" + str(len(hex_data)) + \";\\n\"\n",
    "\n",
    "    # Declare C variable\n",
    "    c_str += \"alignas(8) const unsigned char \" + var_name + \"[] = {\"\n",
    "    hex_array = []\n",
    "    for i, val in enumerate(hex_data):\n",
    "        # Construct string from hex\n",
    "        hex_str = format(val, \"#04x\")\n",
    "        # hex_str = val\n",
    "        # Add formatting so each line stays within 80 characters\n",
    "        if (i + 1) < len(hex_data):\n",
    "            hex_str += \",\"\n",
    "        if (i + 1) % 12 == 0:\n",
    "            hex_str += \"\\n \"\n",
    "        hex_array.append(hex_str)\n",
    "\n",
    "    # Add closing brace\n",
    "    c_str += \"\\n \" + format(\" \".join(hex_array)) + \"\\n};\\n\\n\"\n",
    "\n",
    "    # Close out header guard\n",
    "    c_str += \"#endif //\" + var_name.upper() + \"_H\"\n",
    "\n",
    "    return c_str\n",
    "\n",
    "\n",
    "c_model_name = \"g_model\"\n",
    "with open(c_model_name + \".h\", \"w\") as file:\n",
    "    file.write(hex_to_c_array(tflite_model, c_model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to synchronously open file (file signature not found)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../include/g_model.h\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m converter \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mlite\u001b[38;5;241m.\u001b[39mTFLiteConverter\u001b[38;5;241m.\u001b[39mfrom_keras_model(model)\n\u001b[0;32m      5\u001b[0m tflite_model \u001b[38;5;241m=\u001b[39m converter\u001b[38;5;241m.\u001b[39mconvert()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\saving\\saving_api.py:262\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    255\u001b[0m         filepath,\n\u001b[0;32m    256\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    257\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    258\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    259\u001b[0m     )\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[1;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_sm_saving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\h5py\\_hl\\files.py:562\u001b[0m, in \u001b[0;36mFile.__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[0;32m    553\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[0;32m    554\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[0;32m    555\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[0;32m    556\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[0;32m    557\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[0;32m    558\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    559\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[0;32m    560\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[0;32m    561\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[1;32m--> 562\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\h5py\\_hl\\files.py:235\u001b[0m, in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[0;32m    234\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[1;32m--> 235\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    237\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[1;32mh5py\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to synchronously open file (file signature not found)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.load_model(\"../include/g_model.h\")\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "open(\"converted_model.tflite\", \"wb\").write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
